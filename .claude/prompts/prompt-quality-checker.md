# Prompt Quality Checker - Integrated Mode

This document serves as a reference for prompt quality analysis that happens **directly in your conversations with Claude**.

## How It Works

When you want to check if a prompt is well-written:

1. **Share your draft prompt** with me by saying something like:
   - "Can you check this prompt: [your prompt]"
   - "Is this prompt well-written: [your prompt]"
   - "Analyze this prompt: [your prompt]"

2. **I'll analyze it** based on four criteria:
   - Clarity and Specificity
   - Context Completeness
   - Actionability
   - Technical Accuracy

3. **You'll get feedback** including:
   - Overall quality score (1-10)
   - Detailed scores for each criterion
   - Specific suggestions for improvement
   - An improved version of your prompt

---

## Example Analysis

Here's an example of how I'll analyze your prompts:

**Sample Prompt:**
> "Develop a UI that shows my benchmark results, tests results, filters for tests to run, run index.js, and export results."

### Overall Quality Score: 5/10

### Detailed Analysis:

#### 1. Clarity and Specificity [5/10]
The prompt lists several features but lacks specifics:
- "UI" - What technology? React, Vue, plain HTML?
- "benchmark results" - What format? Where are they stored?
- "tests results" - Same questions apply
- "run index.js" - From the UI? With what parameters?
- "export results" - To what format? CSV, JSON, PDF?

#### 2. Context Completeness [4/10]
Missing important context:
- Current tech stack and project structure
- Where benchmark/test data comes from
- User workflow and use cases
- Any existing UI components to build on
- Design preferences or constraints

#### 3. Actionability [6/10]
The prompt is somewhat actionable - I can build something, but I'll need to make many assumptions or ask clarifying questions. This slows down implementation.

#### 4. Technical Accuracy [6/10]
Uses general terms correctly ("UI", "benchmark results", "export") but lacks technical specificity about the actual implementation.

### Suggestions for Improvement:
- Specify the UI technology/framework to use
- Describe where benchmark and test data comes from (files? API? database?)
- Clarify what "run index.js" means in the context of the UI
- Specify export format(s) needed
- Include any design requirements or existing patterns to follow
- Mention which test runner/framework is being used
- Describe the user workflow (who uses this, when, why)

### Improved Version:

> I need a web-based dashboard for my QA automation project. Here's the context:
>
> **Current Setup:**
> - Playwright tests in `tests/` directory
> - Benchmark results stored in `benchmarks/results/` as JSON files
> - Test results generated by Playwright reporter
> - Main test runner is `index.js` which accepts test path arguments
>
> **Required Features:**
> 1. Display benchmark results in a sortable table (show test name, duration, status, timestamp)
> 2. Display test results with pass/fail status and error messages
> 3. Filter controls to select which tests to run (by path pattern, tags, or status)
> 4. "Run Tests" button that executes `node index.js` with selected filters
> 5. Export both benchmark and test results to CSV format
>
> **Tech Preferences:**
> - Use vanilla HTML/CSS/JavaScript (no framework needed)
> - Simple, functional design - no fancy styling required
> - Should work locally by opening `index.html` in a browser
>
> Can you create this dashboard?

---

## Quick Reference: Signs of a Good Prompt

✅ **Includes specific details** (which component, what error, what version)
✅ **Provides context** (what you've tried, environment, constraints)
✅ **Has clear success criteria** (what "done" looks like)
✅ **Uses precise technical terms** (not "the thing" but "the DataTable component")
✅ **Includes relevant data** (error messages, logs, performance metrics)
✅ **Specifies constraints** (must maintain backwards compatibility, budget, timeline)

❌ **Avoid vague requests** ("make it better", "fix the issue")
❌ **Don't skip context** (assuming Claude knows your codebase)
❌ **Don't use ambiguous language** ("the app", "it doesn't work")
❌ **Don't forget error details** (actual error messages matter)

---

## Usage Tips

- **Before starting a complex task**, share your draft prompt with me first
- Say something like: "Before we start, can you check if this prompt is well-written: [prompt]"
- I'll analyze it and help you refine it before we begin implementation
- This saves time by avoiding back-and-forth clarifications

---

## Common Prompt Issues I'll Help You Fix

1. **Too vague** - "Make it better" → "Optimize the DataTable component to reduce render time from 8s to <2s"
2. **Missing context** - "Fix the bug" → "Fix the 500 error in login endpoint that started after commit abc123"
3. **Unclear scope** - "Add authentication" → "Add JWT-based authentication with email/password login and session persistence"
4. **No success criteria** - "Improve performance" → "Reduce API response time from 2s to <500ms for the /users endpoint"

---

**Ready to use!** Just share any prompt you want me to analyze, and I'll give you feedback before we proceed with the actual task.
