# Prompt Quality Checker - Integrated Mode

This document serves as a reference for prompt quality analysis that happens **directly in your conversations with Claude**.

## Active Configuration

**Automatic Quality Checking is ENABLED with the following settings:**

- **Scope:** Implementation and coding tasks only (feature development, bug fixes, refactoring, etc.)
- **Quality Threshold:** 7/10 - prompts scoring below this will be blocked for improvement
- **Analysis Detail:** Full detailed breakdown with all four criteria, specific issues, and suggestions
- **Simple Requests:** No analysis needed (reading files, running commands, etc.)

## How It Works

### For Implementation/Coding Tasks:

**I will automatically analyze your prompt BEFORE starting work** on:
- Feature development and new functionality
- Bug fixes and debugging tasks
- Code refactoring
- Architecture changes
- Any task involving code modifications

**The Process:**

1. **I analyze your prompt** based on four criteria:
   - Clarity and Specificity
   - Context Completeness
   - Actionability
   - Technical Accuracy

2. **If score ≥ 7/10:** I proceed with the task immediately

3. **If score < 7/10:** I will:
   - Show you the detailed analysis with scores for each criterion
   - Provide specific suggestions for improvement
   - Offer an improved version of your prompt
   - **BLOCK execution** until you refine the prompt

### For Simple/Non-Coding Tasks:

I will proceed immediately without analysis for:
- Reading files
- Running searches
- Explaining code
- Answering questions
- Running commands

### Manual Quality Check:

You can also request a quality check anytime by saying:
- "Can you check this prompt: [your prompt]"
- "Is this prompt well-written: [your prompt]"
- "Analyze this prompt: [your prompt]"

---

## Example Analysis

Here's an example of how I'll analyze your prompts:

**Sample Prompt:**
> "Develop a UI that shows my benchmark results, tests results, filters for tests to run, run index.js, and export results."

### Overall Quality Score: 5/10

### Detailed Analysis:

#### 1. Clarity and Specificity [5/10]
The prompt lists several features but lacks specifics:
- "UI" - What technology? React, Vue, plain HTML?
- "benchmark results" - What format? Where are they stored?
- "tests results" - Same questions apply
- "run index.js" - From the UI? With what parameters?
- "export results" - To what format? CSV, JSON, PDF?

#### 2. Context Completeness [4/10]
Missing important context:
- Current tech stack and project structure
- Where benchmark/test data comes from
- User workflow and use cases
- Any existing UI components to build on
- Design preferences or constraints

#### 3. Actionability [6/10]
The prompt is somewhat actionable - I can build something, but I'll need to make many assumptions or ask clarifying questions. This slows down implementation.

#### 4. Technical Accuracy [6/10]
Uses general terms correctly ("UI", "benchmark results", "export") but lacks technical specificity about the actual implementation.

### Suggestions for Improvement:
- Specify the UI technology/framework to use
- Describe where benchmark and test data comes from (files? API? database?)
- Clarify what "run index.js" means in the context of the UI
- Specify export format(s) needed
- Include any design requirements or existing patterns to follow
- Mention which test runner/framework is being used
- Describe the user workflow (who uses this, when, why)

### Improved Version:

> I need a web-based dashboard for my QA automation project. Here's the context:
>
> **Current Setup:**
> - Playwright tests in `tests/` directory
> - Benchmark results stored in `benchmarks/results/` as JSON files
> - Test results generated by Playwright reporter
> - Main test runner is `index.js` which accepts test path arguments
>
> **Required Features:**
> 1. Display benchmark results in a sortable table (show test name, duration, status, timestamp)
> 2. Display test results with pass/fail status and error messages
> 3. Filter controls to select which tests to run (by path pattern, tags, or status)
> 4. "Run Tests" button that executes `node index.js` with selected filters
> 5. Export both benchmark and test results to CSV format
>
> **Tech Preferences:**
> - Use vanilla HTML/CSS/JavaScript (no framework needed)
> - Simple, functional design - no fancy styling required
> - Should work locally by opening `index.html` in a browser
>
> Can you create this dashboard?

---

## Quick Reference: Signs of a Good Prompt

✅ **Includes specific details** (which component, what error, what version)
✅ **Provides context** (what you've tried, environment, constraints)
✅ **Has clear success criteria** (what "done" looks like)
✅ **Uses precise technical terms** (not "the thing" but "the DataTable component")
✅ **Includes relevant data** (error messages, logs, performance metrics)
✅ **Specifies constraints** (must maintain backwards compatibility, budget, timeline)

❌ **Avoid vague requests** ("make it better", "fix the issue")
❌ **Don't skip context** (assuming Claude knows your codebase)
❌ **Don't use ambiguous language** ("the app", "it doesn't work")
❌ **Don't forget error details** (actual error messages matter)

---

## Usage Tips

- **Automatic analysis is enabled** for implementation tasks - no need to ask!
- I will automatically check your prompt quality before starting any coding work
- If your prompt scores below 7/10, I'll help you improve it before proceeding
- This saves time by ensuring we have all necessary information upfront
- For simple tasks (reading files, searches), I proceed immediately without analysis

---

## Common Prompt Issues I'll Help You Fix

1. **Too vague** - "Make it better" → "Optimize the DataTable component to reduce render time from 8s to <2s"
2. **Missing context** - "Fix the bug" → "Fix the 500 error in login endpoint that started after commit abc123"
3. **Unclear scope** - "Add authentication" → "Add JWT-based authentication with email/password login and session persistence"
4. **No success criteria** - "Improve performance" → "Reduce API response time from 2s to <500ms for the /users endpoint"

---

## Configuration History

**Last Updated:** 2025-12-26

**Current Settings:**
- Auto-check enabled: ✓ Yes
- Scope: Implementation/coding tasks only
- Quality threshold: 7/10 (block if below)
- Analysis detail: Full detailed breakdown
- Simple requests: Skip analysis

**To modify these settings**, ask me to reconfigure the prompt quality checker with your preferred options.

---

**Ready to use!** Automatic quality checking is now active. Just give me your implementation tasks, and I'll analyze them before proceeding if needed.
